{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<!--<badge>--><a href=\"https://colab.research.google.com/github/huggingface/workshops/blob/main/machine-learning-tokyo/03-gradio-demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a><!--</badge>-->"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating a Transformers demo with Gradio\r\n",
        "> exploring Huggingface Transformers library in MLt workshop part 3\r\n",
        "\r\n",
        "- toc: true\r\n",
        "- badges: true\r\n",
        "- comments: true\r\n",
        "- categories: [jupyter]\r\n",
        "- image/chart-preview\r\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating a Transformers demo with Gradio"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## References\n",
        "\n",
        "* https://huggingface.co/blog/gradio-spaces\n",
        "* https://huggingface.co/blog/gradio"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# !pip install transformers gradio sentencepiece"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example 1: Using the Transformers pipeline"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import gradio as gr\r\n",
        "from transformers import pipeline"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "pipe = pipeline(\"text-classification\", model=\"lewtun/xlm-roberta-base-finetuned-marc-en\")"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "pipe(\"The Lord of the Rings is waaay too long to read!!\")"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "label2emoji = {\"terrible\": \"üí©\", \"poor\": \"üòæ\", \"ok\": \"üê±\", \"good\": \"üò∫\", \"great\": \"üòª\"}\r\n",
        "\r\n",
        "def predict(text):\r\n",
        "    preds = pipe(text)[0]\r\n",
        "    return label2emoji[preds[\"label\"]], round(preds[\"score\"], 5)\r\n",
        "\r\n",
        "predict(\"I love this soccer ball\")"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "gradio_ui = gr.Interface(\n",
        "    fn=predict,\n",
        "    title=\"Predicting review scores from customer reviews\",\n",
        "    description=\"Enter some review text about an Amazon product and check what the model predicts for it's star rating.\",\n",
        "    inputs=[\n",
        "        gr.inputs.Textbox(lines=5, label=\"Paste some text here\"),\n",
        "    ],\n",
        "    outputs=[\n",
        "        gr.outputs.Textbox(label=\"Label\"),\n",
        "        gr.outputs.Textbox(label=\"Score\"),\n",
        "    ],\n",
        "    examples=[\n",
        "        [\"My favourite book is Cryptonomicon!\"], [\"ÁßÅ„ÅÆÂ•Ω„Åç„Å™Êú¨„ÅØ„Äå„ÇØ„É™„Éó„Éà„Éé„Éü„Ç≥„É≥„Äç„Åß„Åô\"]\n",
        "    ],\n",
        ")\n",
        "\n",
        "gradio_ui.launch(debug=True)"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example 2: Using the Inference API from the Hugging Face Hub"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from huggingface_hub import InferenceApi\n",
        "\n",
        "text = \"My favourite book is Cryptonomicon!\"\n",
        "inference = InferenceApi(\"lewtun/xlm-roberta-base-finetuned-marc-en\")\n",
        "preds = inference(inputs=text)\n",
        "preds[0]"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "sorted_preds = sorted(preds[0], key=lambda d: d['score'], reverse=True) \n",
        "sorted_preds"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def inference_predict(text):\n",
        "    inference = InferenceApi(\"lewtun/xlm-roberta-base-finetuned-marc-en\")\n",
        "    preds = inference(inputs=text)\n",
        "    sorted_preds = sorted(preds[0], key=lambda d: d['score'], reverse=True)[0]\n",
        "    return label2emoji[sorted_preds[\"label\"]], round(sorted_preds[\"score\"], 5)"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "inference_predict(text)"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "gradio_ui = gr.Interface.load(\n",
        "    name=\"lewtun/xlm-roberta-base-finetuned-marc\",\n",
        "    src=\"huggingface\",\n",
        "    fn=inference_predict,\n",
        "    title=\"Review analysis\",\n",
        "    description=\"Enter some text and check if model detects it's star rating.\",\n",
        "    inputs=[\n",
        "        gr.inputs.Textbox(lines=5, label=\"Paste some text here\"),\n",
        "    ],\n",
        "    outputs=[\n",
        "        gr.outputs.Textbox(label=\"Label\"),\n",
        "        gr.outputs.Textbox(label=\"Score\"),\n",
        "    ],\n",
        "    examples=[\n",
        "        [\"My favourite book is Cryptonomicon!\"], [\"ÁßÅ„ÅÆÂ•Ω„Åç„Å™Êú¨„ÅØ„Äå„ÇØ„É™„Éó„Éà„Éé„Éü„Ç≥„É≥„Äç„Åß„Åô\"]\n",
        "    ],\n",
        ")\n",
        "\n",
        "gradio_ui.launch(debug=True)"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {}
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "9f835d35ef2d7d572ed1f1be271ae903cca02f9a46b282db81c294a2d4ce247b"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}