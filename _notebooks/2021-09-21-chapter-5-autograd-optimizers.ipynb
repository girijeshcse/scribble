{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "colab": {
      "name": "2021-09-21-chapter-5-autograd-optimizers.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "rXxERKhZAvMA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 5 The Magic of Autograd and Optimizers\r\n",
        "> A summary of chapter 5 part 2 of Deep learning with PyTorch\r\n",
        "\r\n",
        "- toc: true\r\n",
        "- badges: true\r\n",
        "- comments: true\r\n",
        "- categories: [jupyter]\r\n",
        "- image: image/chart-preview"
      ],
      "metadata": {
        "id": "KIJSy6Eyor4h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The magic of Autograd\n",
        "\n",
        "In [part 1](https://girijeshcse.github.io/scribble/jupyter/2121/09/14/chapter-5.html) we saw that how we can train a linear model with backpropagation. In nutshell what we computed the gradient of a composition of functions—the model and the loss—with respect to their innermost parameters (w and b) by propagating derivatives backward using the chain rule.\n",
        "      \n",
        "The basic requirement here is that all functions we’re dealing\n",
        "with can be differentiated analytically but as we increase the depth of the model, writing the analytical expression for the derivatives is not that easy.\n",
        "\n",
        "This is when PyTorch tensors come to the rescue, with a PyTorch component called\n",
        "*autograd*.\n",
        "\n",
        "PyTorch tensors can remember where they come from, in terms of the operations and parent tensors that originated them, and they can automatically provide the chain of derivatives of such operations with respect to their inputs. This means **PyTorch will automatically provide the gradient of that expression with respect to its input parameters.**\n"
      ],
      "metadata": {
        "id": "-H9h7-Vkphix"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Computing the gradient automatically \n",
        "\n",
        "Lets rewrite out code using autograd."
      ],
      "metadata": {
        "id": "V3MDYHGTvQSj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "source": [
        "%matplotlib inline\r\n",
        "import numpy as np\r\n",
        "import torch\r\n",
        "torch.set_printoptions(edgeitems=2)"
      ],
      "outputs": [],
      "metadata": {
        "id": "8UZ36r5_ARM7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "source": [
        "t_c = torch.tensor([0.5, 14.0, 15.0, 28.0, 11.0, 8.0,\r\n",
        "                    3.0, -4.0, 6.0, 13.0, 21.0])\r\n",
        "t_u = torch.tensor([35.7, 55.9, 58.2, 81.9, 56.3, 48.9,\r\n",
        "                    33.9, 21.8, 48.4, 60.4, 68.4])\r\n",
        "t_un = 0.1 * t_u"
      ],
      "outputs": [],
      "metadata": {
        "id": "SCEDiX5PARNB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Recalling our model and loss function"
      ],
      "metadata": {
        "id": "f1I_EObawXkV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "source": [
        "def model(t_u, w, b):\r\n",
        "    return w * t_u + b"
      ],
      "outputs": [],
      "metadata": {
        "id": "lu3sWd-gARNC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "source": [
        "def loss_fn(t_p, t_c):\r\n",
        "    squared_diffs = (t_p - t_c)**2\r\n",
        "    return squared_diffs.mean()"
      ],
      "outputs": [],
      "metadata": {
        "id": "86Sq10MkARND"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s again initialize a parameters tensor:"
      ],
      "metadata": {
        "id": "LJit4AyUwfKP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "source": [
        "params = torch.tensor([1.0, 0.0], requires_grad=True)"
      ],
      "outputs": [],
      "metadata": {
        "id": "dbp2Z2iCARND"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The *requires_grad=True* argument is telling PyTorch to track the entire family tree of tensors resulting from operations on params."
      ],
      "metadata": {
        "id": "_2ujHQtownKg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "source": [
        "params.grad is None"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cqDrDZ7KARNE",
        "outputId": "38498b99-b0db-4331-de23-61afaa382d5e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "source": [
        "loss = loss_fn(model(t_u, *params), t_c)\r\n",
        "loss.backward()\r\n",
        "\r\n",
        "params.grad"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([4517.2969,   82.6000])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NLmBqIYLARNF",
        "outputId": "3b539e36-61cb-450f-a5e4-182846dea877"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "At this point, the grad attribute of params contains the derivatives of the loss with respect to each element of params."
      ],
      "metadata": {
        "id": "9g-L_6GDxCQC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\r\n",
        "## The forward graph and backward graph of the model as computed with autograd\r\n",
        "\r\n",
        "\r\n",
        "![](my_icons/ch55.JPG)\r\n",
        "\r\n",
        "When we compute our loss while the parameters w and b require gradients, in addition to performing the actual computation, PyTorch creates the autograd graph with the operations (in black circles) as nodes, as shown in the top row of figure above. When we call *loss.backward()*, PyTorch traverses this graph in the reverse direction to compute the gradients, as shown by the arrows in the bottom row of the figure.\r\n",
        "\r\n"
      ],
      "metadata": {
        "id": "tuQCP0ktxMhA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Accumulating Grad Funtion\n",
        "\n",
        "Calling backward will lead derivatives to accumulate at leaf nodes. So if backward was called earlier, the loss is evaluated again, backward is called again (as in any training loop), and the gradient at each leaf is accumulated (that is, summed) on top of the one computed at the previous iteration, which leads to an ***incorrect value*** for the gradient. In order to prevent this from occurring, ***we need to zero the gradient explicitly at each iteration***. We can do this easily using the in-place zero_ method:"
      ],
      "metadata": {
        "id": "Cu4Osc8sxLwx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "source": [
        "if params.grad is not None:\r\n",
        "    params.grad.zero_()"
      ],
      "outputs": [],
      "metadata": {
        "id": "9TPwPqjTARNH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "source": [
        "def training_loop(n_epochs, learning_rate, params, t_u, t_c):\r\n",
        "    for epoch in range(1, n_epochs + 1):\r\n",
        "        if params.grad is not None:  # This could be done at any point in the loop prior to calling loss.backward().\r\n",
        "            params.grad.zero_()\r\n",
        "        \r\n",
        "        t_p = model(t_u, *params) \r\n",
        "        loss = loss_fn(t_p, t_c)\r\n",
        "        loss.backward()\r\n",
        "        \r\n",
        "        with torch.no_grad():  # This is a somewhat cumbersome bit of code, but as we’ll see in the next section, it’s not an issue in practice.\r\n",
        "            params -= learning_rate * params.grad\r\n",
        "\r\n",
        "        if epoch % 500 == 0:\r\n",
        "            print('Epoch %d, Loss %f' % (epoch, float(loss)))\r\n",
        "            \r\n",
        "    return params"
      ],
      "outputs": [],
      "metadata": {
        "id": "3SIEr0R8ARNI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "source": [
        "training_loop(\r\n",
        "    n_epochs = 5000, \r\n",
        "    learning_rate = 1e-2, \r\n",
        "    params = torch.tensor([1.0, 0.0], requires_grad=True), # <1> \r\n",
        "    t_u = t_un, # <2> \r\n",
        "    t_c = t_c)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 500, Loss 7.860115\n",
            "Epoch 1000, Loss 3.828538\n",
            "Epoch 1500, Loss 3.092191\n",
            "Epoch 2000, Loss 2.957698\n",
            "Epoch 2500, Loss 2.933134\n",
            "Epoch 3000, Loss 2.928648\n",
            "Epoch 3500, Loss 2.927830\n",
            "Epoch 4000, Loss 2.927679\n",
            "Epoch 4500, Loss 2.927652\n",
            "Epoch 5000, Loss 2.927647\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([  5.3671, -17.3012], requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "46JQYXT6ARNJ",
        "outputId": "1e7d2ed5-1531-4979-9f8a-65d16a70b835"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Making this more better"
      ],
      "metadata": {
        "id": "7XJagZOO0NYC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "NXrs3egsARNK"
      }
    }
  ]
}